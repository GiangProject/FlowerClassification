# src/app/main.py

# ===================================================================
# ‚≠êÔ∏è C√ÄI ƒê·∫∂T TH∆Ø VI·ªÜN & IMPORTS (ƒêI·ªÄU PH·ªêI)
# ===================================================================
print("--- ƒêang c√†i ƒë·∫∑t/ki·ªÉm tra th∆∞ vi·ªán ph·ª• thu·ªôc ---")
# C·∫ßn ch·∫°y l·ªánh n√†y n·∫øu m√¥i tr∆∞·ªùng ch∆∞a c√†i ƒë·∫∑t
# !pip install -q torchsummary opencv-python scikit-learn
print("--- Ho√†n t·∫•t c√†i ƒë·∫∑t th∆∞ vi·ªán ---")

import os
import sys
import json
import scipy.io
import pandas as pd
import numpy as np
import torch
import torch.optim as optim
from torch.optim import lr_scheduler
import copy
import time
from tqdm.auto import tqdm # Th√™m tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import gc
from sklearn.metrics import classification_report, accuracy_score # Th√™m accuracy_score
from sklearn.preprocessing import label_binarize

# --- IMPORTS T·ª™ SRC ---
# S·ª¨A L·ªñI MODULE NOT FOUND: B·ªé COMMENT D√íNG TH√äM PATH T∆Ø∆†NG ƒê·ªêI
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))) 
from config import DEVICE, NUM_CLASSES, PROJECT_PATH
from preprocessing import get_dataloaders, base_train_transforms, strong_train_transforms, valid_test_transforms 
from eda import visualize_all_102_flowers, plot_color_distribution, analyze_image_size
from model_training import build_model, build_model_b1, build_model_vit, train_phase
from evaluation import (
    evaluate_all_metrics_combined, plot_roc_curve_ovr, plot_training_history,
    analyze_occlusion_sensitivity, analyze_tsne, plot_confusion_matrix_subsets,
    plot_per_class_accuracy
)

# ===================================================================
# ‚≠êÔ∏è SETUP, T·∫¢I D·ªÆ LI·ªÜU & T·∫†O DATAFRAME G·ªêC (BLOCK 1: LOGIC KH·ªûI T·∫†O)
# ===================================================================
# --- G·∫ÆN DRIVE V√Ä THI·∫æT L·∫¨P TH∆Ø M·ª§C ---
from google.colab import drive
try: drive.mount('/content/drive')
except: pass
os.makedirs(PROJECT_PATH, exist_ok=True); os.chdir(PROJECT_PATH)
os.makedirs('data', exist_ok=True); os.makedirs('models', exist_ok=True)

def ensure_data_integrity():
    """T·∫£i v√† ki·ªÉm tra t√≠nh to√†n v·∫πn c·ªßa d·ªØ li·ªáu g·ªëc (·∫£nh v√† file .mat)."""
    MAT_FILE_PATH = 'data/imagelabels.mat'

    if not os.path.exists('jpg') or not os.path.exists(MAT_FILE_PATH) or (os.path.exists('jpg') and len(os.listdir('jpg/')) < 8000):
        print("--- üì• ƒêang t·∫£i v√† ki·ªÉm tra D·ªØ li·ªáu G·ªëc ---")
        # ƒê√É S·ª¨A L·ªñI C√ö PH√ÅP: Thay th·∫ø !wget b·∫±ng os.system('wget ...')
        os.system('wget -q https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz -P data/')
        os.system('wget -q https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat -P data/')
        os.system('wget -q https://www.robots.ox.ac.uk/~vgg/data/flowers/102/setid.mat -P data/')
        os.system('wget -q https://raw.githubusercontent.com/udacity/pytorch_challenge/master/cat_to_name.json -P data/')
        time.sleep(3)

        if os.path.exists('jpg'): os.system('rm -rf jpg')

        if os.path.exists('data/102flowers.tgz'):
             os.system('tar -xzf data/102flowers.tgz')
             print("Gi·∫£i n√©n ·∫£nh ho√†n t·∫•t.")

        time.sleep(2)

        if not os.path.exists(MAT_FILE_PATH) or len(os.listdir('jpg/')) < 8000:
            print("‚ùå L·ªñI TRUY C·∫¨P DRIVE: D·ªØ li·ªáu b·ªã thi·∫øu sau nhi·ªÅu l·∫ßn th·ª≠.")
            raise FileNotFoundError("Kh√¥ng th·ªÉ ƒë·∫£m b·∫£o t√≠nh to√†n v·∫πn c·ªßa d·ªØ li·ªáu.")

    print(f"‚úÖ D·ªØ li·ªáu ƒë√£ s·∫µn s√†ng. S·ªë l∆∞·ª£ng file ·∫£nh: {len(os.listdir('jpg/'))}")

ensure_data_integrity()

# --- T·∫†O DATAFRAME V√Ä CLASS NAMES ---
image_labels = scipy.io.loadmat('data/imagelabels.mat')['labels'][0]
set_ids = scipy.io.loadmat('data/setid.mat')
with open('data/cat_to_name.json', 'r') as f: cat_to_name = json.load(f)

image_files = sorted(os.listdir('jpg/'))
df = pd.DataFrame({'filepath': 'jpg/' + pd.Series(image_files), 'label': image_labels - 1})
df['split'] = ''; df.loc[set_ids['trnid'][0] - 1, 'split'] = 'train'; df.loc[set_ids['valid'][0] - 1, 'split'] = 'valid'; df.loc[set_ids['tstid'][0] - 1, 'split'] = 'test'
df['class_id'] = (df['label'] + 1).astype(str); df['flower_name'] = df['class_id'].map(cat_to_name); df = df.drop(columns=['class_id'])
class_names_df = df.drop_duplicates(subset=['label']).sort_values('label'); class_names = class_names_df['flower_name'].tolist()

print(f"\nThi·∫øt b·ªã s·ª≠ d·ª•ng: {DEVICE}"); print("--- DataFrame df v√† class_names ƒë√£ s·∫µn s√†ng ---")
# df.info() # Gi·ªØ nguy√™n df.info() n·∫øu c·∫ßn hi·ªÉn th·ªã th√¥ng tin dataframe

# ===================================================================
# ‚≠êÔ∏è TH·ª∞C THI EDA (BLOCK 2: G·ªåI C√ÅC H√ÄM EDA)
# ===================================================================
visualize_all_102_flowers(df)
print("\n--- Bi·ªÉu ƒë·ªì ph√¢n b·ªë 20 lo√†i hoa c√≥ nhi·ªÅu ·∫£nh nh·∫•t ---")
plt.figure(figsize=(12, 8))
sns.countplot(y='flower_name', data=df, order=df['flower_name'].value_counts().iloc[:20].index, palette='viridis')
plt.title('Ph√¢n b·ªë 20 lo√†i hoa h√†ng ƒë·∫ßu'); plt.xlabel('S·ªë l∆∞·ª£ng ·∫£nh'); plt.ylabel('T√™n lo√†i hoa'); plt.show()
plot_color_distribution(df)
analyze_image_size(df)

# ===================================================================
# ‚≠êÔ∏è TH·ª∞C THI HU·∫§N LUY·ªÜN M√î H√åNH (BLOCK 4: G·ªåI C√ÅC H√ÄM TRAIN_PHASE)
# ===================================================================
all_histories = {}
criterion = torch.nn.CrossEntropyLoss()

# --- TH·ª¨ NGHI·ªÜM 1: BASELINE B0 ---
print("üöÄ B·∫Øt ƒë·∫ßu Th·ª≠ nghi·ªám 1: Baseline Augmentation")
EXP_NAME = "exp1_baseline"; MODEL_SAVE_PATH = f'models/{EXP_NAME}.pth'
train_loader, valid_loader, test_loader_b0_base, test_df_baseline = get_dataloaders(df, base_train_transforms, batch_size=32)
model_exp1 = build_model().to(DEVICE)
optimizer_phase1 = optim.Adam(model_exp1.classifier.parameters(), lr=1e-3); scheduler_phase1 = lr_scheduler.ReduceLROnPlateau(optimizer_phase1, mode='min', patience=3)
model_exp1, history_exp1_p1 = train_phase(model_exp1, criterion, optimizer_phase1, scheduler_phase1, {'train': train_loader, 'valid': valid_loader}, num_epochs=15, patience=3, model_save_path=MODEL_SAVE_PATH)
for param in model_exp1.parameters(): param.requires_grad = True
optimizer_ft = optim.Adam(model_exp1.parameters(), lr=1e-5); scheduler_ft = lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode='min', patience=2)
model_exp1, history_exp1_p2 = train_phase(model_exp1, criterion, optimizer_ft, scheduler_ft, {'train': train_loader, 'valid': valid_loader}, num_epochs=10, patience=3, model_save_path=MODEL_SAVE_PATH)
all_histories['Baseline (B0)'] = {key: history_exp1_p1[key] + history_exp1_p2[key] for key in history_exp1_p1}
print(f"\n‚úÖ Ho√†n t·∫•t Th·ª≠ nghi·ªám {EXP_NAME} v√† ƒë√£ l∆∞u l·ªãch s·ª≠!")

# --- TH·ª¨ NGHI·ªÜM 2: STRONG AUG B0 ---
print("üöÄ B·∫Øt ƒë·∫ßu Th·ª≠ nghi·ªám 2: Strong Augmentation")
EXP_NAME = "exp2_strong_aug"; MODEL_SAVE_PATH = f'models/{EXP_NAME}.pth'
train_loader, valid_loader, test_loader_b0_strong, test_df_strong = get_dataloaders(df, strong_train_transforms, batch_size=32)
model_exp2 = build_model().to(DEVICE)
optimizer_phase1 = optim.Adam(model_exp2.classifier.parameters(), lr=1e-4); scheduler_phase1 = lr_scheduler.ReduceLROnPlateau(optimizer_phase1, mode='min', patience=5)
model_exp2, history_exp2_p1 = train_phase(model_exp2, criterion, optimizer_phase1, scheduler_phase1, {'train': train_loader, 'valid': valid_loader}, num_epochs=15, patience=5, model_save_path=MODEL_SAVE_PATH)
for param in model_exp2.parameters(): param.requires_grad = True
optimizer_ft = optim.Adam(model_exp2.parameters(), lr=1e-4); scheduler_ft = lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode='min', patience=2)
model_exp2, history_exp2_p2 = train_phase(model_exp2, criterion, optimizer_ft, scheduler_ft, {'train': train_loader, 'valid': valid_loader}, num_epochs=15, patience=3, model_save_path=MODEL_SAVE_PATH)
all_histories['Strong Aug (B0)'] = {key: history_exp2_p1[key] + history_exp2_p2[key] for key in history_exp2_p1}
print(f"\n‚úÖ Ho√†n t·∫•t Th·ª≠ nghi·ªám {EXP_NAME} v√† ƒë√£ l∆∞u l·ªãch s·ª≠!")

# --- TH·ª¨ NGHI·ªÜM 3/4: B1 & B1 CONTINUED ---
print("üöÄ B·∫Øt ƒë·∫ßu Th·ª≠ nghi·ªám 3/4: EfficientNet-B1")
EXP_NAME = "exp4_b1_continued"; PREVIOUS_BEST_MODEL = 'models/exp3_efficientnet_b1.pth'; MODEL_SAVE_PATH = f'models/{EXP_NAME}.pth'
train_loader, valid_loader, test_loader_b1, test_df_b1 = get_dataloaders(df, strong_train_transforms, batch_size=32)
model_exp3 = build_model_b1().to(DEVICE)
optimizer_phase1 = optim.Adam(model_exp3.classifier.parameters(), lr=1e-4); scheduler_phase1 = lr_scheduler.ReduceLROnPlateau(optimizer_phase1, mode='min', patience=5)
model_exp3, history_exp3_p1 = train_phase(model_exp3, criterion, optimizer_phase1, scheduler_phase1, {'train': train_loader, 'valid': valid_loader}, num_epochs=15, patience=5, model_save_path=PREVIOUS_BEST_MODEL)
for param in model_exp3.parameters(): param.requires_grad = True
optimizer_ft = optim.Adam(model_exp3.parameters(), lr=1e-4); scheduler_ft = lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode='min', patience=2)
model_exp3, history_exp3_p2 = train_phase(model_exp3, criterion, optimizer_ft, scheduler_ft, {'train': train_loader, 'valid': valid_loader}, num_epochs=15, patience=3, model_save_path=PREVIOUS_BEST_MODEL)
model_to_continue = build_model_b1().to(DEVICE); model_to_continue.load_state_dict(torch.load(PREVIOUS_BEST_MODEL, map_location=DEVICE))
for param in model_to_continue.parameters(): param.requires_grad = True
optimizer_continue = optim.Adam(model_to_continue.parameters(), lr=1e-4); scheduler_continue = lr_scheduler.ReduceLROnPlateau(optimizer_continue, mode='min', patience=2)
final_model, history_exp4 = train_phase(model_to_continue, criterion, optimizer_continue, scheduler_continue, {'train': train_loader, 'valid': valid_loader}, num_epochs=15, patience=4, model_save_path=MODEL_SAVE_PATH)
all_histories['Final Model (B1)'] = {key: history_exp3_p1[key] + history_exp3_p2[key] + history_exp4[key] for key in history_exp3_p1}
print(f"\n‚úÖ Ho√†n t·∫•t Th·ª≠ nghi·ªám {EXP_NAME} v√† ƒë√£ l∆∞u l·ªãch s·ª≠!")

# --- TH·ª¨ NGHI·ªÜM 5: ViT ---
print("üöÄ B·∫Øt ƒë·∫ßu Th·ª≠ nghi·ªám 5: ViT-B/16")
EXP_NAME = "exp_vit"; MODEL_SAVE_PATH = f'models/{EXP_NAME}.pth'
train_loader, valid_loader, test_loader_vit, test_df_vit = get_dataloaders(df, strong_train_transforms, batch_size=32)
model_exp_vit = build_model_vit().to(DEVICE)
optimizer_phase1 = optim.Adam(model_exp_vit.heads.head.parameters(), lr=1e-4); scheduler_phase1 = lr_scheduler.ReduceLROnPlateau(optimizer_phase1, mode='min', patience=3)
model_exp_vit, history_vit_p1 = train_phase(model_exp_vit, criterion, optimizer_phase1, scheduler_phase1, {'train': train_loader, 'valid': valid_loader}, num_epochs=15, patience=5, model_save_path=MODEL_SAVE_PATH)
for param in model_exp_vit.parameters(): param.requires_grad = True
optimizer_ft = optim.Adam(model_exp_vit.parameters(), lr=5e-5); scheduler_ft = lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode='min', patience=3)
model_exp_vit, history_vit_p2 = train_phase(model_exp_vit, criterion, optimizer_ft, scheduler_ft, {'train': train_loader, 'valid': valid_loader}, num_epochs=15, patience=4, model_save_path=MODEL_SAVE_PATH)
all_histories['ViT-B/16'] = {key: history_vit_p1[key] + history_vit_p2[key] for key in history_vit_p1}
print(f"\n‚úÖ Ho√†n t·∫•t Th·ª≠ nghi·ªám {EXP_NAME} v√† ƒë√£ l∆∞u l·ªãch s·ª≠!")

torch.cuda.empty_cache(); gc.collect()

# ===================================================================
# ‚≠êÔ∏è ƒê√ÅNH GI√Å V√Ä PH√ÇN T√çCH T·ªîNG K·∫æT (BLOCK 5, 6, 7: G·ªåI C√ÅC H√ÄM EVALUATION)
# ===================================================================
best_model_labels, best_model_preds, best_model_probs = None, None, None
best_acc_so_far = -1.0
best_model_name = ""
results_summary = {}

# --- C·∫§U H√åNH ƒê√ÅNH GI√Å ---
MODEL_PATHS_TO_EVAL = {
    'Baseline (B0)': ('models/exp1_baseline.pth', 'EfficientNet-B0', base_train_transforms, 32),
    'Strong Aug (B0)': ('models/exp2_strong_aug.pth', 'EfficientNet-B0', strong_train_transforms, 32),
    'Final Model (B1)': ('models/exp4_b1_continued.pth', 'EfficientNet-B1', strong_train_transforms, 32),
    'ViT-B/16': ('models/exp_vit.pth', 'ViT-B/16', strong_train_transforms, 32),
}

# --- TH·ª∞C THI ƒê√ÅNH GI√Å T·ªîNG K·∫æT V√Ä T√åM M√î H√åNH T·ªêT NH·∫§T ---
for exp_name, (path, model_type, train_aug_func, batch_size) in MODEL_PATHS_TO_EVAL.items():
    metrics, labels, preds, probs = evaluate_all_metrics_combined(df, class_names, path, model_type, train_aug_func, batch_size)

    if metrics:
        results_summary[exp_name] = metrics

        # LOGIC T·ª∞ ƒê·ªòNG CH·ªåN M√î H√åNH T·ªêT NH·∫§T
        if metrics.get('test_accuracy', 0) > best_acc_so_far:
            best_acc_so_far = metrics['test_accuracy']
            best_model_name = exp_name
            best_model_labels, best_model_preds, best_model_probs = labels, preds, probs # L∆∞u k·∫øt qu·∫£ m√¥ h√¨nh t·ªët nh·∫•t

# --- 5b: HI·ªÇN TH·ªä B·∫¢NG K·∫æT QU·∫¢ V√Ä BI·ªÇU ƒê·ªí SO S√ÅNH ---
if results_summary:
    print(f"\n‚úÖ M√¥ h√¨nh t·ªët nh·∫•t ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ ph√¢n t√≠ch chi ti·∫øt l√†: {best_model_name} (Accuracy: {best_acc_so_far:.4f})")
    df_results = pd.DataFrame(results_summary).T; results_df = df_results.sort_values('test_accuracy', ascending=False)

    print("\n" + "="*80); print("                    üìä B·∫¢NG SO S√ÅNH HI·ªÜU SU·∫§T T·ªîNG TH·ªÇ C√ÅC M√î H√åNH"); print("="*80)
    formatters = {'train_accuracy': '{:,.2%}'.format, 'test_accuracy': '{:,.2%}'.format, 'test_auc': '{:,.3f}'.format, 'test_f1-score': '{:,.3f}'.format, 'test_precision': '{:,.3f}'.format, 'test_recall': '{:,.3f}'.format}
    print(results_df.to_string(formatters=formatters)); print("="*80)

    # Tr·ª±c quan h√≥a Train vs Test Accuracy
    df_acc = results_df[['train_accuracy', 'test_accuracy']].reset_index().rename(columns={'index': 'model'})
    df_acc_melted = pd.melt(df_acc, id_vars='model', value_vars=['train_accuracy', 'test_accuracy'], var_name='Metric', value_name='Accuracy')
    plt.figure(figsize=(14, 7)); sns.barplot(data=df_acc_melted, x='model', y='Accuracy', hue='Metric', palette='Set1')
    plt.title('So s√°nh ƒê·ªô ch√≠nh x√°c Train vs. Test (Ph√¢n t√≠ch Overfitting)', fontsize=16); plt.xticks(rotation=15); plt.grid(axis='y', linestyle='--', alpha=0.7); plt.show()

    # Tr·ª±c quan h√≥a Test F1-Score vs Test AUC
    df_perf_f1_auc = results_df[['test_f1-score', 'test_auc']].reset_index().rename(columns={'index': 'model'})
    df_perf_f1_auc_melted = pd.melt(df_perf_f1_auc, id_vars='model', value_vars=['test_f1-score', 'test_auc'], var_name='Metric', value_name='Score')
    plt.figure(figsize=(14, 7)); sns.barplot(data=df_perf_f1_auc_melted, x='model', y='Score', hue='Metric', palette='Set2')
    plt.title('So s√°nh F1-Score v√† AUC tr√™n T·∫≠p Test', fontsize=16); plt.xticks(rotation=15); plt.grid(axis='y', linestyle='--', alpha=0.7); plt.show()

    # TR·ª∞C QUAN H√ìA ROC CURVE
    if best_model_labels is not None and best_model_probs is not None:
        print("\n\n" + "="*80); print(f"          üìà TR·ª∞C QUAN H√ìA ƒê∆Ø·ªúNG CONG ROC ({best_model_name})"); print("="*80)
        y_test_binarized = label_binarize(best_model_labels, classes=range(NUM_CLASSES)); y_score = np.array(best_model_probs)
        plot_roc_curve_ovr(y_test_binarized, y_score, class_names, best_model_name)
    
    # TR·ª∞C QUAN H√ìA L·ªäCH S·ª¨ HU·∫§N LUY·ªÜN
    if all_histories:
        print("\n\n" + "="*80); print("                     üìà L·ªäCH S·ª¨ HU·∫§N LUY·ªÜN C√ÅC M√î H√åNH"); print("="*80)
        for model_name, history in all_histories.items(): plot_training_history(history, model_name)


# --- PH√ÇN T√çCH CHI TI·∫æT M√î H√åNH T·ªêT NH·∫§T (BLOCK 6 & 7) ---
if best_model_labels is not None:
    print("\n" + "="*80); print("          üî¨ B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH DI·ªÑN GI·∫¢I M√î H√åNH T·ªêT NH·∫§T"); print("="*80)

    # 1. T·∫£i l·∫°i m√¥ h√¨nh t·ªët nh·∫•t
    if best_model_name == 'ViT-B/16':
        BEST_MODEL_PATH = 'models/exp_vit.pth'; model_to_visualize = build_model_vit().to(DEVICE)
    elif best_model_name == 'Final Model (B1)':
        BEST_MODEL_PATH = 'models/exp4_b1_continued.pth'; model_to_visualize = build_model_b1().to(DEVICE)
    elif best_model_name == 'Strong Aug (B0)':
        BEST_MODEL_PATH = 'models/exp2_strong_aug.pth'; model_to_visualize = build_model().to(DEVICE)
    else:
        BEST_MODEL_PATH = 'models/exp1_baseline.pth'; model_to_visualize = build_model().to(DEVICE)

    try:
        model_to_visualize.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=DEVICE))
        model_to_visualize.eval()
        
        # 2. B√°o c√°o ph√¢n lo·∫°i chi ti·∫øt
        print("\nüìä B√°o c√°o Ph√¢n lo·∫°i Chi ti·∫øt cho M√¥ h√¨nh T·ªët nh·∫•t:\n")
        print(classification_report(best_model_labels, best_model_preds, target_names=class_names))
        
        # 3. Tr·ª±c quan h√≥a ƒë·ªô ch√≠nh x√°c t·ª´ng l·ªõp
        plot_per_class_accuracy(best_model_labels, best_model_preds, class_names, best_model_name)
        
        # 4. Ph√¢n t√≠ch Occlusion Sensitivity
        analyze_occlusion_sensitivity(df, model_to_visualize, best_model_name, class_names)
        
        # 5. Ph√¢n t√≠ch t-SNE
        analyze_tsne(df, model_to_visualize, best_model_name, class_names)
        
        # 6. Confusion Matrix chia nh·ªè
        plot_confusion_matrix_subsets(best_model_labels, best_model_preds, class_names, best_model_name)

    except Exception as e:
        print(f"‚ùå L·ªói t·∫£i ho·∫∑c ph√¢n t√≠ch chi ti·∫øt m√¥ h√¨nh: {e}")
