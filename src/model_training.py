# src/model_training.py

import torch
import torch.nn as nn
import torchvision.models as models
from torchsummary import summary
import copy
import time
from tqdm.auto import tqdm
import torch.optim as optim
from torch.optim import lr_scheduler

# ÄÃƒ Sá»¬A Lá»–I: DÃ¹ng Absolute Import (config)
from config import INPUT_SIZE, NUM_CLASSES, DEVICE 

# --- MODEL BUILDERS ---
def build_model(model_name='EfficientNet-B0', num_classes=NUM_CLASSES):
    """
    HÃ m xÃ¢y dá»±ng mÃ´ hÃ¬nh chung (B0, B1, ViT) tá»« kiáº¿n trÃºc tiá»n huáº¥n luyá»‡n ImageNet.
    Thá»±c hiá»‡n Ä‘Ã³ng bÄƒng cÃ¡c tham sá»‘ Feature Extractor vÃ  thay tháº¿ lá»›p Head (Classifier) cuá»‘i cÃ¹ng.
    """
    model_name = model_name.lower(); model = None
    if 'b0' in model_name: model = models.efficientnet_b0(weights='IMAGENET1K_V1')
    elif 'b1' in model_name: model = models.efficientnet_b1(weights='IMAGENET1K_V1')
    elif 'vit' in model_name: model = models.vit_b_16(weights='IMAGENET1K_V1')
    else: raise ValueError(f"Kiáº¿n trÃºc {model_name} khÃ´ng Ä‘Æ°á»£c há»— trá»£ trong hÃ m build_model.")

    # ÄÃ³ng bÄƒng táº¥t cáº£ tham sá»‘
    for param in model.parameters(): param.requires_grad = False

    # Thay tháº¿ lá»›p phÃ¢n loáº¡i cuá»‘i cÃ¹ng
    if 'vit' in model_name:
        num_ftrs = model.heads.head.in_features
        model.heads.head = torch.nn.Linear(num_ftrs, num_classes)
    else: # EfficientNet (B0/B1)
        num_ftrs = model.classifier[1].in_features
        model.classifier[1] = torch.nn.Linear(num_ftrs, num_classes)

    print(f"\n--- Kiáº¿n trÃºc {model_name.upper()} ---")
    try:
        # Bá» qua summary cho ViT do khÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i torchsummary
        if 'vit' not in model_name: summary(model.to(DEVICE), (3, INPUT_SIZE, INPUT_SIZE))
        else: print("TÃ³m táº¯t kiáº¿n trÃºc bá»‹ bá» qua do khÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i torchsummary.")
    except Exception: print("Lá»—i khi hiá»ƒn thá»‹ summary.")

    return model.to(DEVICE)

def build_model_b1(num_classes=NUM_CLASSES): 
    """HÃ m táº¡o mÃ´ hÃ¬nh EfficientNet-B1."""
    return build_model('EfficientNet-B1', num_classes)
    
def build_model_vit(num_classes=NUM_CLASSES): 
    """HÃ m táº¡o mÃ´ hÃ¬nh ViT-B/16."""
    return build_model('ViT-B/16', num_classes)

# --- TRAINING PHASE ---
def train_phase(model, criterion, optimizer, scheduler, dataloaders, num_epochs=20, patience=5, model_save_path=None):
    """
    Thá»±c hiá»‡n vÃ²ng láº·p huáº¥n luyá»‡n chÃ­nh vá»›i cÃ¡c giai Ä‘oáº¡n Train vÃ  Validation.
    Bao gá»“m logic Early Stopping vÃ  lÆ°u láº¡i trá»ng sá»‘ mÃ´ hÃ¬nh tá»‘t nháº¥t.
    """
    device = next(model.parameters()).device
    scaler = torch.cuda.amp.GradScaler() # Sá»­ dá»¥ng GradScaler cho Mixed Precision Training (tÄƒng tá»‘c GPU)
    best_model_wts = copy.deepcopy(model.state_dict()); best_acc = 0.0
    epochs_no_improve = 0
    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
    start_time = time.time()

    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1}/{num_epochs}\n{'-'*20}")
        for phase in ['train','valid']:
            model.train() if phase=='train' else model.eval()
            running_loss, running_corrects = 0.0, 0
            if phase=='train': torch.cuda.empty_cache()

            for inputs, labels in tqdm(dataloaders[phase], desc=f"{phase} phase"):
                inputs, labels = inputs.to(device), labels.to(device); optimizer.zero_grad()
                
                with torch.set_grad_enabled(phase=='train'):
                    with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):
                        outputs = model(inputs); _, preds = torch.max(outputs, 1); loss = criterion(outputs, labels)
                    if phase=='train': 
                        scaler.scale(loss).backward()
                        scaler.step(optimizer)
                        scaler.update()

                running_loss += loss.item() * inputs.size(0); running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            epoch_acc = running_corrects.double().cpu().item() / len(dataloaders[phase].dataset)

            if phase=='train': 
                history['train_loss'].append(epoch_loss); history['train_acc'].append(epoch_acc)
            else:
                history['val_loss'].append(epoch_loss); history['val_acc'].append(epoch_acc)
                # Logic Early Stopping vÃ  lÆ°u mÃ´ hÃ¬nh tá»‘t nháº¥t
                if epoch_acc > best_acc:
                    best_acc = epoch_acc; epochs_no_improve = 0
                    best_model_wts = copy.deepcopy(model.state_dict())
                    if model_save_path: torch.save(model.state_dict(), model_save_path); print(f"âœ¨ Best model saved! Val Acc: {best_acc:.4f}")
                else: epochs_no_improve += 1

            print(f"{phase.capitalize()} Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f}")

        if scheduler and isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau): scheduler.step(epoch_loss)

        if epochs_no_improve >= patience: 
            print(f"ðŸ›‘ Early stopping triggered after {patience} epochs"); break
        print()

    time_elapsed = time.time() - start_time
    print(f"Training complete in {time_elapsed//60:.0f}m {time_elapsed%60:.0f}s"); print(f"Best Val Acc: {best_acc:.4f}")
    model.load_state_dict(best_model_wts)
    return model, history
